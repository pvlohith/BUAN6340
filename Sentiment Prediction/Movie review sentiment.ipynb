{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Number of unique words in the train dataset\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   19,  178,   32],\n",
       "       [   0,    0,    0, ...,   16,  145,   95],\n",
       "       [   0,    0,    0, ...,    7,  129,  113],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4, 3586,    2],\n",
       "       [   0,    0,    0, ...,   12,    9,   23],\n",
       "       [   0,    0,    0, ...,  204,  131,    9]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "51725\n"
     ]
    }
   ],
   "source": [
    "# Number of unique words in the test dataset\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 238.71 words (176.493674)\n"
     ]
    }
   ],
   "source": [
    "# Average length of review\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X_train]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAKvCAYAAABOPYjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3X+snXd9J/j3B9vYS4aCGQwicWgyVVrdYO0EGlFaPDP1MECgqwldbZfcVI3BFiYzcJWKrkLoXQm23YtIdtoKvAxRMtdtIpEL7HRaIhGGyVDvdi1+NKaTAYdbJiYNcJs0cXG2BQNO4nz3j/uYXieOk9jf+Nzj+3pJR+ecz/k+z/k8sn301tff53mqtRYAAODUPGfUDQAAwJlAsAYAgA4EawAA6ECwBgCADgRrAADoQLAGAIAOBGsAAOhAsAYAgA4EawAA6GD1qBs4WS9+8YvbeeedN+o2AJ6xr3zlK3/TWtsw6j5OJ7/ZwLh6Jr/ZYxuszzvvvOzdu3fUbQA8Y1X1rVH3cLr5zQbG1TP5zbYUBAAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAFWiKo6t6p2V9V8Vd1VVVcN9Q9U1V9V1Z3D481LtnlfVe2vqm9U1RuX1C8Zavur6ppRHA/AcrN61A0AcNo8muQ3Wmt/XlXPT/KVqrp9+Oz3Wmv/ZungqrowyWVJXpHk7CT/uap+evj4o0len2QhyR1VdWtr7eun5SgAlikz1gArRGvt/tbanw+vv5dkPsk5J9jk0iSfaK0dbq39ZZL9SV49PPa31u5prT2c5BPDWDit5ubmsmnTpqxatSqbNm3K3NzcqFtihROsAVagqjovySuTfHkovbuqvlpVu6pq/VA7J8l3lmy2MNSerA6nzdzcXKanp7Nz58786Ec/ys6dOzM9PS1cM1KCNcAKU1X/IMkfJvn11trfJflYkp9KclGS+5P8ztGhx9m8naD++O/ZUVV7q2rvgQMHuvQOR83MzGR2djZbtmzJmjVrsmXLlszOzmZmZmbUrbGCCdYAK0hVrcliqP54a+0/JElr7YHW2pHW2mNJbsziUo9kcSb63CWbb0xy3wnqx2it3dBau7i1dvGGDRv6Hwwr2vz8fDZv3nxMbfPmzZmfnx9RR/A0gvXw34IPVtW+JbVPLjl7/N6qunOon1dVP1zy2fVLtvnZqvracAb5R6qqhvqLqur2qrp7eF7/xC4AOFXD7+5skvnW2u8uqb9sybBfTnL09/7WJJdV1dqqOj/JBUn+LMkdSS6oqvOr6rlZPMHx1tNxDHDUxMRE9uzZc0xtz549mZiYGFFH8PSuCvIHSf7PJDcfLbTW3nr0dVX9TpK/XTL+m621i46zn48l2ZHkS0luS3JJks8muSbJ51trHxou2XRNkvc+s8N4+s675jPP1q6f4N4P/dJp+y6Ap+G1SX4tydeOTogk+c0kk1V1URaXc9yb5J1J0lq7q6o+leTrWbyiyLtaa0eSpKreneRzSVYl2dVau+t0HghMT09n+/btmZ2dzebNm7Nnz55s377dUhBG6imDdWvtT4eTXJ5gmP34n5P88xPtY5gN+YnW2heH9zcneUsWg/WlSX5xGHpTkv87z2KwBlipWmt7cvz10bedYJuZJE9IKq212060HTzbJicnkyRTU1OZn5/PxMREZmZmflyHUTjV61j/kyQPtNbuXlI7v6r+S5K/S/K/ttb+3yyeLb6wZMzSM8hf2lq7P1m8FFRVveQUewIAVoDJyUlBmmXlVIP1ZJKl17W5P8nLW2vfraqfTfLHVfWKPM0zyJ9KVe3I4nKSvPzlLz+JdgEA4Nlx0lcFqarVSf7HJJ88WhtuIvDd4fVXknwzyU9ncYZ645LNl55B/sDRE2eG5wef7DudYQ4AwHJ1Kpfb+xdJ/qK19uMlHlW1oapWDa//URbPIL9nWOrxvap6zbAu+4oknx42uzXJ1uH11iV1AAAYG0/ncntzSb6Y5GeqaqGqtg8fXZZjl4EkyT9N8tWq+q9J/n2SK1trB4fP/lWSf5fFW+J+M4snLibJh5K8vqruTvL64T0AAIyVp3NVkOOeFdBae9txan+YxRsPHG/83iSbjlP/bpLXPVUfAACwnLnzIgAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdPCUwbqqdlXVg1W1b0ntA1X1V1V15/B485LP3ldV+6vqG1X1xiX1S4ba/qq6Zkn9/Kr6clXdXVWfrKrn9jxAAAA4HZ7OjPUfJLnkOPXfa61dNDxuS5KqujDJZUleMWzzb6tqVVWtSvLRJG9KcmGSyWFsklw77OuCJA8l2X4qBwQAAKPwlMG6tfanSQ4+zf1dmuQTrbXDrbW/TLI/yauHx/7W2j2ttYeTfCLJpVVVSf55kn8/bH9Tkrc8w2MAAICRO5U11u+uqq8OS0XWD7VzknxnyZiFofZk9X+Y5P9rrT36uDoAAIyVkw3WH0vyU0kuSnJ/kt8Z6nWcse0k6sdVVTuqam9V7T1w4MAz6xgAAJ5FJxWsW2sPtNaOtNYeS3JjFpd6JIszzucuGboxyX0nqP9NkhdW1erH1Z/se29orV3cWrt4w4YNJ9M6AAA8K04qWFfVy5a8/eUkR68YcmuSy6pqbVWdn+SCJH+W5I4kFwxXAHluFk9wvLW11pLsTvI/DdtvTfLpk+kJAABGafVTDaiquSS/mOTFVbWQ5P1JfrGqLsriso17k7wzSVprd1XVp5J8PcmjSd7VWjsy7OfdST6XZFWSXa21u4aveG+ST1TV/57kvySZ7XZ0AABwmjxlsG6tTR6n/KTht7U2k2TmOPXbktx2nPo9+fulJAAAMJbceREAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaYIWoqnOrandVzVfVXVV11VB/UVXdXlV3D8/rh3pV1Ueqan9VfbWqXrVkX1uH8XdX1dZRHRPAciJYA6wcjyb5jdbaRJLXJHlXVV2Y5Jokn2+tXZDk88P7JHlTkguGx44kH0sWg3iS9yf5uSSvTvL+o2EcYCUTrAFWiNba/a21Px9efy/JfJJzklya5KZh2E1J3jK8vjTJzW3Rl5K8sKpeluSNSW5vrR1srT2U5PYkl5zGQwFYlgRrgBWoqs5L8sokX07y0tba/cli+E7ykmHYOUm+s2SzhaH2ZHWAFU2wBlhhquofJPnDJL/eWvu7Ew09Tq2doP7479lRVXurau+BAwdOrlmAMSJYA6wgVbUmi6H64621/zCUHxiWeGR4fnCoLyQ5d8nmG5Pcd4L6MVprN7TWLm6tXbxhw4a+BwKwDAnWACtEVVWS2STzrbXfXfLRrUmOXtlja5JPL6lfMVwd5DVJ/nZYKvK5JG+oqvXDSYtvGGoAK9rqUTcAwGnz2iS/luRrVXXnUPvNJB9K8qmq2p7k20l+ZfjstiRvTrI/yQ+SvD1JWmsHq+q3k9wxjPut1trB03MIAMuXYA2wQrTW9uT466OT5HXHGd+SvOtJ9rUrya5+3QGMP0tBAACgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAGEtzc3PZtGlTVq1alU2bNmVubm7ULbHCCdYAwNiZm5vLVVddlUOHDqW1lkOHDuWqq64SrhkpwRoAGDtXX311Vq1alV27duXw4cPZtWtXVq1alauvvnrUrbGCCdYAwNhZWFjIzTffnC1btmTNmjXZsmVLbr755iwsLIy6NVYwwRoAADoQrAGAsbNx48Zs3bo1u3fvziOPPJLdu3dn69at2bhx46hbYwUTrAGAsXPdddfl0UcfzbZt27Ju3bps27Ytjz76aK677rpRt8YKJlgDAGNncnIyH/7wh3PWWWclSc4666x8+MMfzuTk5Ig7YyVbPeoGAABOxuTkpCDNsmLGGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOnjKYF1Vu6rqwarat6T2f1TVX1TVV6vqj6rqhUP9vKr6YVXdOTyuX7LNz1bV16pqf1V9pKpqqL+oqm6vqruH5/XPxoECAMCz6enMWP9BkkseV7s9yabW2n+f5L8led+Sz77ZWrtoeFy5pP6xJDuSXDA8ju7zmiSfb61dkOTzw3sAgBOam5vLpk2bsmrVqmzatClzc3OjbokV7imDdWvtT5McfFztP7XWHh3efinJxhPto6peluQnWmtfbK21JDcnecvw8aVJbhpe37SkDgBwXHNzc5mens7OnTvzox/9KDt37sz09LRwzUj1WGO9Lclnl7w/v6r+S1X9P1X1T4baOUkWloxZGGpJ8tLW2v1JMjy/pENPAMAZbGZmJrOzs9myZUvWrFmTLVu2ZHZ2NjMzM6NujRVs9alsXFXTSR5N8vGhdH+Sl7fWvltVP5vkj6vqFUnqOJu3k/i+HVlcTpKXv/zlJ9c0ADD25ufns3nz5mNqmzdvzvz8/Ig6glOYsa6qrUn+hyS/OizvSGvtcGvtu8PrryT5ZpKfzuIM9dLlIhuT3De8fmBYKnJ0yciDT/adrbUbWmsXt9Yu3rBhw8m2DgCMuYmJiezZs+eY2p49ezIxMTGijuAkg3VVXZLkvUn+ZWvtB0vqG6pq1fD6H2XxJMV7hiUe36uq1wxXA7kiyaeHzW5NsnV4vXVJHQDguKanp7N9+/bs3r07jzzySHbv3p3t27dnenp61K2xgj3lUpCqmkvyi0leXFULSd6fxauArE1y+3DVvC8NVwD5p0l+q6oeTXIkyZWttaMnPv6rLF5h5L/L4prso+uyP5TkU1W1Pcm3k/xKlyMDAM5Yk5OTSZKpqanMz89nYmIiMzMzP67DKDxlsG6tHe9v6OyTjP3DJH/4JJ/tTbLpOPXvJnndU/UBALDU5OSkIM2y4s6LAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQCMpbm5uWzatCmrVq3Kpk2bMjc3N+qWWOFWj7oBAIBnam5uLtPT05mdnc3mzZuzZ8+ebN++PUkyOTk54u5YqcxYAwBjZ2ZmJrOzs9myZUvWrFmTLVu2ZHZ2NjMzM6NujRVMsAYAxs78/Hw2b958TG3z5s2Zn58fUUcgWAMAY2hiYiJ79uw5prZnz55MTEyMqCMQrAGAMTQ9PZ3t27dn9+7deeSRR7J79+5s374909PTo26NFczJiwDA2Dl6guLU1FTm5+czMTGRmZkZJy4yUoI1ADCWJicnBWmWFUtBAACgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAYCxNDc3l02bNmXVqlXZtGlT5ubmRt0SK5zrWAMAY2dubi7T09OZnZ3N5s2bs2fPnmzfvj1JXNuakTFjDQCMnZmZmczOzmbLli1Zs2ZNtmzZktnZ2czMzIy6NVYwwRoAGDvz8/PZvHnzMbXNmzdnfn5+RB2BYA0AjKGJiYns2bPnmNqePXsyMTExoo5AsAYAxtD09HS2b9+e3bt355FHHsnu3buzffv2TE9Pj7o1VjAnLwIAY2dycjJf+MIX8qY3vSmHDx/O2rVr8453vMOJi4yUGWsAYOzMzc3lM5/5TD772c/m4Ycfzmc/+9l85jOfcck9RkqwBgDGjquCsBwJ1gDA2Jmfn8/CwsIxN4hZWFhwVRBGyhprAGDsnH322Xnve9+bj3/84z++Qcyv/uqv5uyzzx51a6xgZqwBgLHUWjvhezjdBGsAYOzcd999ue666zI1NZV169Zlamoq1113Xe67775Rt8YKZikIADB2JiYmsnHjxuzbt+/Htd27d7tBDCNlxhoAGDtuEMNyZMYaABg7R28EMzU1lfn5+UxMTGRmZsYNYhgpM9YAK0RV7aqqB6tq35LaB6rqr6rqzuHx5iWfva+q9lfVN6rqjUvqlwy1/VV1zek+DjhqcnIy+/bty5EjR7Jv3z6hmpETrAFWjj9Icslx6r/XWrtoeNyWJFV1YZLLkrxi2ObfVtWqqlqV5KNJ3pTkwiSTw1iAFc9SEIAVorX2p1V13tMcfmmST7TWDif5y6ran+TVw2f7W2v3JElVfWIY+/XO7QKMHTPWALy7qr46LBVZP9TOSfKdJWMWhtqT1QFWPMEaYGX7WJKfSnJRkvuT/M5Qr+OMbSeoP0FV7aiqvVW198CBAz16BVjWBGuAFay19kBr7Uhr7bEkN+bvl3ssJDl3ydCNSe47Qf14+76htXZxa+3iDRs29G8eYJkRrAFWsKp62ZK3v5zk6BVDbk1yWVWtrarzk1yQ5M+S3JHkgqo6v6qem8UTHG89nT0DLFdOXgRYIapqLskvJnlxVS0keX+SX6yqi7K4nOPeJO9MktbaXVX1qSyelPhokne11o4M+3l3ks8lWZVkV2vtrtN8KADLkmANsEK01o53kd/ZE4yfSTJznPptSW7r2BrAGcFSEAAA6ECwBgCADgRrAADoQLAGAIAOBGsAAOhAsAYAgA4EawAA6ECwBgCADgRrAADoQLAGAIAOBGsAAOhAsAYAgA4EawAA6ECwBgDG0tzcXDZt2pRVq1Zl06ZNmZubG3VLrHCrR90AAMAzNTc3l+np6czOzmbz5s3Zs2dPtm/fniSZnJwccXesVGasAYCxMzMzk8svvzxTU1NZt25dpqamcvnll2dmZmbUrbGCmbEGAMbO17/+9Rw6dCi7du368Yz1tm3b8q1vfWvUrbGCCdYAwNh57nOfm9e+9rWZmprK/Px8JiYm8trXvjb333//qFtjBbMUBAAYO4cPH84nP/nJbNu2Ld/73veybdu2fPKTn8zhw4dH3RormGANAIydtWvX5q1vfWt27dqV5z//+dm1a1fe+ta3Zu3ataNujRVMsAYAxs7DDz+cL3zhC9m5c2d+9KMfZefOnfnCF76Qhx9+eNStsYJZYw0AjJ0LL7wwb3nLW45ZY3355Zfnj//4j0fdGiuYGWsAYOxMT0/nlltuOWbG+pZbbsn09PSoW2MFe1rBuqp2VdWDVbVvSe1FVXV7Vd09PK8f6lVVH6mq/VX11ap61ZJttg7j766qrUvqP1tVXxu2+UhVVc+DBADOLJOTk5mZmTnmOtYzMzNuDsNIPd0Z6z9Icsnjatck+Xxr7YIknx/eJ8mbklwwPHYk+ViyGMSTvD/JzyV5dZL3Hw3jw5gdS7Z7/HcBABxjcnIy+/bty5EjR7Jv3z6hmpF7WsG6tfanSQ4+rnxpkpuG1zclecuS+s1t0ZeSvLCqXpbkjUlub60dbK09lOT2JJcMn/1Ea+2LrbWW5OYl+wIAgLFwKmusX9pauz9JhueXDPVzknxnybiFoXai+sJx6gAAMDaejZMXj7c+up1E/Yk7rtpRVXurau+BAwdOoUUAAOjrVIL1A8MyjgzPDw71hSTnLhm3Mcl9T1HfeJz6E7TWbmitXdxau3jDhg2n0DoAAPR1KsH61iRHr+yxNcmnl9SvGK4O8pokfzssFflckjdU1frhpMU3JPnc8Nn3quo1w9VArliyLwAAGAtP93J7c0m+mORnqmqhqrYn+VCS11fV3UleP7xPktuS3JNkf5Ibk/zrJGmtHUzy20nuGB6/NdSS5F8l+XfDNt9M8tlTPzQA4Ew2NzeXTZs2ZdWqVdm0aVPm5uZG3RIr3NO682Jr7cmuX/O644xtSd71JPvZlWTXcep7k2x6Or0AAMzNzWV6ejqzs7PZvHlz9uzZk+3btyeJy+4xMu68CACMnZmZmczOzmbLli1Zs2ZNtmzZktnZ2czMzIy6NVYwwRoAGDvz8/PZvHnzMbXNmzdnfn5+RB2BYA0AjKGJiYns2bPnmNqePXsyMTExoo5AsAYAxtD09HS2b9+e3bt355FHHsnu3buzffv2TE9Pj7o1VrCndfIiAMBycvQExampqczPz2diYiIzMzNOXGSkBGsAYCxNTk4K0iwrloIAAGPJdaxZbgRrAGDszM3N5aqrrsqhQ4fSWsuhQ4dy1VVXCdeMlGANAIydq6++OqtWrcquXbty+PDh7Nq1K6tWrcrVV1896tZYwQRrAGDsLCws5O1vf3umpqaybt26TE1N5e1vf3sWFhZG3RormJMXAYCx9Pu///u55ZZbfnxL88svv3zULbHCmbEGAMbO6tWr8/DDDx9Te/jhh7N6tTlDRsffPgBg7Bw5ciTPec5zsm3btnz729/Oy1/+8jznOc/JkSNHRt0aK5gZawBg7Fx44YV55zvfmbPOOitJctZZZ+Wd73xnLrzwwhF3xkomWAMAY2d6ejq33HJLdu7cmR/96EfZuXNnbrnlFrc0Z6QsBQEAxo5bmrMcCdYAwFhyS3OWG0tBAACgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAYCxNDc3l02bNmXVqlXZtGlT5ubmRt0SK5zrWAMAY2dubi7T09OZnZ3N5s2bs2fPnmzfvj1JXNuakTFjDQCMnZmZmczOzmbLli1Zs2ZNtmzZktnZ2czMzIy6NVYwwRoAGDvz8/NZWFg4ZinIwsJC5ufnR90aK5hgDQCMnbPPPjtTU1M5dOhQkuTQoUOZmprK2WefPeLOWMkEawBg7PzgBz/I97///UxNTeV73/tepqam8v3vfz8/+MEPRt0aK5hgDQCMnYMHD+bqq6/Orl278vznPz+7du3K1VdfnYMHD466NVYwwRoAGEtbtmzJvn37cuTIkezbty9btmwZdUuscII1ADB2Nm7cmCuuuCK7d+/OI488kt27d+eKK67Ixo0bR90aK5hgDQCMneuuuy5HjhzJtm3bsnbt2mzbti1HjhzJddddN+rWWMEEawBg7ExOTubDH/5wzjrrrFRVzjrrrHz4wx92cxhGyp0XAYCxNDk5KUizrJixBgCADgRrAADoQLAGAIAOBGsAAOhAsAYAgA4EawBgLE1NTWXdunWpqqxbty5TU1OjbokVTrAGAMbO1NRUrr/++nzwgx/MoUOH8sEPfjDXX3+9cM1ICdYAwNi58cYbc+211+Y973lPnve85+U973lPrr322tx4442jbo0VTLAGAMbO4cOHc+WVVx5Tu/LKK3P48OERdQSCNQAwhtauXZvrr7/+mNr111+ftWvXjqgjcEtzAGAMveMd78h73/veJIsz1ddff33e+973PmEWG04nwRoAGDs7d+5Mkvzmb/5mfuM3fiNr167NlVde+eM6jIJgDQCMpZ07dwrSLCvWWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAGNpamoq69atS1Vl3bp1bmfOyAnWAMDYmZqayvXXX58PfvCDOXToUD74wQ/m+uuvF64ZKcEaABg7N954Y6699tq85z3vyfOe97y85z3vybXXXpsbb7xx1K2xggnWAMDYOXz48BPusnjllVfm8OHDI+oI3CAGABhDa9euzY4dO3LnnXdmfn4+ExMTueiii7J27dpRt8YKJlgDAGPnn/2zf5aPf/zjec5znpPHHnss8/Pzueuuu/KGN7xh1K2xglkKAgCMnb179yZJquqY56N1GAXBGgAYOwcPHszP//zPZ/Xqxf98X716dX7+538+Bw8eHHFnrGSCNQAwlr785S8fc7m9L3/5y6NuiRVOsAYAxtK6devyyle+MmvWrMkrX/nKrFu3btQtscI5eREAGEs//OEPc/nll+fBBx/MS17ykvzwhz8cdUuscGasAYCxs3bt2vzCL/xCHnrooTz22GN56KGH8gu/8Asut8dICdYAwNh5xzvekS9+8YtZv359nvOc52T9+vX54he/mHe84x2jbo0VTLAGAMbO0dnpv/7rv85jjz2Wv/7rv/7xLDaMimANAIydq6++Oi94wQvyJ3/yJ3n44YfzJ3/yJ3nBC16Qq6++etStsYIJ1gDA2FlYWMjb3va2TE1NZd26dZmamsrb3va2LCwsjLo1VjBXBQEAxtLv//7vZ25uLps3b86ePXsyOTk56pZY4QRrAGDsrF69OocOHcq2bdvyrW99Kz/5kz+ZQ4cO/fhOjDAK/vYBAGPn0Ucfzfe///18//vfT5Lce++9o20IYo01AAB0IVgDAEAHgjUAAHSNmeAZAAAT5UlEQVQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDrBBVtauqHqyqfUtqL6qq26vq7uF5/VCvqvpIVe2vqq9W1auWbLN1GH93VW0dxbEALEcnHayr6meq6s4lj7+rql+vqg9U1V8tqb95yTbvG36kv1FVb1xSv2So7a+qa071oAA4rj9Icsnjatck+Xxr7YIknx/eJ8mbklwwPHYk+ViyGMSTvD/JzyV5dZL3Hw3jACvdSQfr1to3WmsXtdYuSvKzSX6Q5I+Gj3/v6GettduSpKouTHJZkldk8Yf931bVqqpaleSjWfwRvzDJ5DAWgI5aa3+a5ODjypcmuWl4fVOStyyp39wWfSnJC6vqZUnemOT21trB1tpDSW7PE8M6wIrU65bmr0vyzdbat6rqycZcmuQTrbXDSf6yqvZncbYjSfa31u5Jkqr6xDD26516A+DJvbS1dn+StNbur6qXDPVzknxnybiFofZkdYAVr9ca68uSzC15/+5hTd6uJf9F6EcaYHwcb5aknaD+xB1U7aiqvVW198CBA12bA1iOTjlYV9Vzk/zLJP/XUPpYkp9KclGS+5P8ztGhx9ncjzTAaD0wLPHI8PzgUF9Icu6ScRuT3HeC+hO01m5orV3cWrt4w4YN3RsHWG56zFi/Kcmft9YeSJLW2gOttSOttceS3Ji/X+7hRxpg+bk1ydEre2xN8ukl9SuGq4O8JsnfDktGPpfkDVW1fvgfyTcMNYAVr0ewnsySZSBHZz4Gv5zk6GWdbk1yWVWtrarzs3im+Z8luSPJBVV1/jD7fdkwFoCOqmouyReT/ExVLVTV9iQfSvL6qro7yeuH90lyW5J7kuzP4iTJv06S1trBJL+dxd/uO5L81lADWPFO6eTFqnpeFn+I37mkfF1VXZTF5Rz3Hv2stXZXVX0qiyclPprkXa21I8N+3p3FGY9VSXa11u46lb4AeKLW2uSTfPS644xtSd71JPvZlWRXx9YAzginFKxbaz9I8g8fV/u1E4yfSTJznPptWZwdAQCAseTOiwAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdrB51A2ey8675zGn9vns/9Eun9fsAAPh7ZqwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoINTDtZVdW9Vfa2q7qyqvUPtRVV1e1XdPTyvH+pVVR+pqv1V9dWqetWS/Wwdxt9dVVtPtS8AADides1Yb2mtXdRau3h4f02Sz7fWLkjy+eF9krwpyQXDY0eSjyWLQTzJ+5P8XJJXJ3n/0TAOAADj4NlaCnJpkpuG1zclecuS+s1t0ZeSvLCqXpbkjUlub60dbK09lOT2JJc8S70BAEB3PYJ1S/KfquorVbVjqL20tXZ/kgzPLxnq5yT5zpJtF4bak9WPUVU7qmpvVe09cOBAh9YBAKCP1R328drW2n1V9ZIkt1fVX5xgbB2n1k5QP7bQ2g1JbkiSiy+++AmfAwDAqJzyjHVr7b7h+cEkf5TFNdIPDEs8Mjw/OAxfSHLuks03JrnvBHUAABgLpxSsq+qsqnr+0ddJ3pBkX5Jbkxy9ssfWJJ8eXt+a5Irh6iCvSfK3w1KRzyV5Q1WtH05afMNQAwCAsXCqS0FemuSPqurovm5prf3HqrojyaeqanuSbyf5lWH8bUnenGR/kh8keXuStNYOVtVvJ7ljGPdbrbWDp9gbAACcNqcUrFtr9yT5x8epfzfJ645Tb0ne9ST72pVk16n0AwAAo+LOiwAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANDB6lE3AADweFX1rG7bWjvp/cOTEawBgGXnqYLvicKz0MyoWAoCAIydJwvPQjWjZMYaABhLR0N0VQnULAtmrAFIVd1bVV+rqjurau9Qe1FV3V5Vdw/P64d6VdVHqmp/VX21ql412u4BlgfBGoCjtrTWLmqtXTy8vybJ51trFyT5/PA+Sd6U5ILhsSPJx057pwDLkGANwJO5NMlNw+ubkrxlSf3mtuhLSV5YVS8bRYMAy4lgDUCStCT/qaq+UlU7htpLW2v3J8nw/JKhfk6S7yzZdmGoHaOqdlTV3qrae+DAgWexdYDlwcmLACTJa1tr91XVS5LcXlV/cYKxx7vO2RPOHGut3ZDkhiS5+OKLnVkGnPHMWAOQ1tp9w/ODSf4oyauTPHB0icfw/OAwfCHJuUs235jkvtPXLcDyJFgDrHBVdVZVPf/o6yRvSLIvya1Jtg7Dtib59PD61iRXDFcHeU2Svz26ZARgJbMUBICXJvmj4U52q5Pc0lr7j1V1R5JPVdX2JN9O8ivD+NuSvDnJ/iQ/SPL2098ywPIjWAOscK21e5L84+PUv5vkdceptyTvOg2tAYwVS0EAAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADpYPeoGAIAz34te9KI89NBDz9r+q+pZ2e/69etz8ODBZ2XfnHkEawDgWffQQw+ltTbqNp6xZyuwc2ayFAQAADoQrAEAoAPBGgAAOjjpYF1V51bV7qqar6q7quqqof6BqvqrqrpzeLx5yTbvq6r9VfWNqnrjkvolQ21/VV1zaocEAACn36mcvPhokt9orf15VT0/yVeq6vbhs99rrf2bpYOr6sIklyV5RZKzk/znqvrp4eOPJnl9koUkd1TVra21r59CbwAAcFqddLBurd2f5P7h9feqaj7JOSfY5NIkn2itHU7yl1W1P8mrh8/2t9buSZKq+sQwVrAGAGBsdFljXVXnJXllki8PpXdX1VeraldVrR9q5yT5zpLNFobak9UBAGBsnHKwrqp/kOQPk/x6a+3vknwsyU8luSiLM9q/c3TocTZvJ6gf77t2VNXeqtp74MCBU20dAAC6OaVgXVVrshiqP95a+w9J0lp7oLV2pLX2WJIb8/fLPRaSnLtk841J7jtB/Qlaaze01i5urV28YcOGU2kdAAC6OpWrglSS2STzrbXfXVJ/2ZJhv5xk3/D61iSXVdXaqjo/yQVJ/izJHUkuqKrzq+q5WTzB8daT7QsAAEbhVK4K8tokv5bka1V151D7zSSTVXVRFpdz3JvknUnSWrurqj6VxZMSH03yrtbakSSpqncn+VySVUl2tdbuOoW+AADgtDuVq4LsyfHXR992gm1mkswcp37bibYDAIDlzp0XAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgA8EaAAA6EKwBAKADwRoAADoQrAEAoAPBGgAAOhCsAQCgg9WjbgAAOPO19/9E8oEXjLqNZ6y9/ydG3QJjRLAGAJ519b/9XVpro27jGauqtA+MugvGhaUgAADQgWANAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgWANAAAdCNYAANCBYA0AAB24pTkAcFpU1ahbeMbWr18/6hYYI4I1APCsa609a/uuqmd1//B0WQoCAAAdCNYAANCBYA0AAB0I1gAA0IFgDQAAHQjWAADQgcvtnUHOu+Yzp/X77v3QL53W7wMAWM7MWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAPwjFXVJVX1jaraX1XXjLofgOVg9agbAGC8VNWqJB9N8vokC0nuqKpbW2tfH21nnEmq6lkd31p7RuPh6RCsAXimXp1kf2vtniSpqk8kuTSJYE03gi/jyFIQAJ6pc5J8Z8n7haEGsKIJ1gA8U8f7P/cnTC9W1Y6q2ltVew8cOHAa2gIYLcEagGdqIcm5S95vTHLf4we11m5orV3cWrt4w4YNp605gFERrAF4pu5IckFVnV9Vz01yWZJbR9wTwMg5eRGAZ6S19mhVvTvJ55KsSrKrtXbXiNsCGDnBGoBnrLV2W5LbRt0HwHJiKQgAAHQgWAMAQAeCNQAAdCBYAwBAB05e5KSdd81nTtt33fuhXzpt3wUAcDLMWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAeCNQAAdCBYAwBAB4I1AAB0IFgDAEAHgjUAAHQgWAMAQAerR93AUVV1SZIPJ1mV5N+11j404pZYRs675jOn9fvu/dAvndbvAwDG37KYsa6qVUk+muRNSS5MMllVF462KwAAePqWRbBO8uok+1tr97TWHk7yiSSXjrgnAAB42pZLsD4nyXeWvF8YagAAMBaWyxrrOk6tPWFQ1Y4kO4a336+qbzyD73hxkr85id7GhePrqK49Xd/0Y2fyn9+ZfGzJyR3fTz4bjSxnX/nKV/6mqr416j44Y53pvzOM1tP+zV4uwXohyblL3m9Mct/jB7XWbkhyw8l8QVXtba1dfHLtLX+Ob7ydycd3Jh9bcuYfXy+ttQ2j7oEzl3+HLBfLZSnIHUkuqKrzq+q5SS5LcuuIewIAgKdtWcxYt9Yerap3J/lcFi+3t6u1dteI2wIAgKdtWQTrJGmt3ZbktmfxK05qCckYcXzj7Uw+vjP52JIz//hgHPh3yLJQrT3hHEEAAOAZWi5rrAEAYKytiGBdVZdU1Teqan9VXTPqfk5WVd1bVV+rqjurau9Qe1FV3V5Vdw/P64d6VdVHhmP+alW9arTdH6uqdlXVg1W1b0ntGR9LVW0dxt9dVVtHcSzH8yTH94Gq+qvhz+/Oqnrzks/eNxzfN6rqjUvqy+7vblWdW1W7q2q+qu6qqquG+hnx53eC4zsj/vzgTHK831oYqdbaGf3I4smQ30zyj5I8N8l/TXLhqPs6yWO5N8mLH1e7Lsk1w+trklw7vH5zks9m8Rrhr0ny5VH3/7i+/2mSVyXZd7LHkuRFSe4ZntcPr9eP+thOcHwfSPK/HGfshcPfy7VJzh/+vq5arn93k7wsyauG189P8t+GYzgj/vxOcHxnxJ+fx//f3t2D2FGFYRz/v2CwUCGCGOwMYh+thICkWrWKFgFTqIiFRSysbQRtbLS1kAgR/CCgQQvRpLPyA0XwI01AEXFJihUVBMH4WMy5EJZ711yY69459/+D4Q5nh2Ue3mH25c7ZOW49bfPutW5u+7ltwjfWvS+Xfhw40/bPAA9fM/5GBp8CB6vqjv04wXmSfALs7BpeNssDwIUkO0l+BS4AD67+7P/bgnyLHAfeSfJXkh+ASwzX7Vpeu0m2k3zV9v8ALjKslNpF/fbIt8ik6if1ZMl7rbRym9BY97RceoDzVfVlW4US4FCSbRgaAuD2Nj7F3MtmmWLGZ9p0iNdnUyWYcL6quhO4B/iMDuu3Kx90Vj9J0rg2obG+ruXSJ+JoknuBh4BTVXX/Hsf2lHtRlqllfBW4CzgCbAMvt/FJ5quqm4F3gWeT/L7XoXPGppivq/pJksa3CY31dS2XPgVJfmmfV4BzDI+aL8+meLTPK+3wKeZeNsukMia5nORqkn+A1xjqBxPMV1UHGJrON5O814a7qd+8fD3VT5K0GpvQWHexXHpV3VRVt8z2gS3gW4Yss7cpPAG83/Y/AB5vb2S4D/ht9ph+jS2b5WNgq6pubY/lt9rYWto1x/0RhvrBkO/Rqrqxqg4DdwOfs6bXblUVcBq4mOSVa37URf0W5eulfpKk1VmblRdXJf0sl34IODf8zecG4K0kH1XVF8DZqnoK+Ak40Y7/kOFtDJeAP4En//9TXqyq3gaOAbdV1c/A88BLLJElyU5VvcjQwAC8kGQt/ollQb5jVXWEYTrAj8DTAEm+q6qzwPfA38CpJFfb71nHa/co8BjwTVV93caeo5/6Lcp3spP6Sd2Yd69Ncnp/z0qbzJUXJUmSpBFswlQQSZIkaeVsrCVJkqQR2FhLkiRJI7CxliRJkkZgYy1JkiSNwMZakiRJGoGNtSRJkjQCG2tJkiRpBP8CGwhHEy7+nnMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b897c07550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plots showing the distribution of length of reviews\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(result)\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top 5000 words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500 #Only the first 500 words in a review are being considered for easier analysis\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50848\n"
     ]
    }
   ],
   "source": [
    "#Modelling with a basic logistic regression to get a base line accuracy\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train,y_train)\n",
    "y_predict=model1.predict(X_test)\n",
    "accuracy_test=accuracy_score(y_test,y_predict)\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train, y_train)\n",
    "all_predictions = spam_detect_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, all_predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/12\n",
      "25000/25000 [==============================] - 190s 8ms/step - loss: 0.7226 - acc: 0.5012 - val_loss: 0.7080 - val_acc: 0.5093\n",
      "Epoch 2/12\n",
      "25000/25000 [==============================] - 184s 7ms/step - loss: 0.7038 - acc: 0.5098 - val_loss: 0.6947 - val_acc: 0.5216\n",
      "Epoch 3/12\n",
      "25000/25000 [==============================] - 190s 8ms/step - loss: 0.6942 - acc: 0.5232 - val_loss: 0.6892 - val_acc: 0.5327\n",
      "Epoch 4/12\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6908 - acc: 0.5252 - val_loss: 0.6874 - val_acc: 0.5339\n",
      "Epoch 5/12\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.6899 - acc: 0.5286 - val_loss: 0.6873 - val_acc: 0.5329\n",
      "Epoch 6/12\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6906 - acc: 0.5275 - val_loss: 0.6905 - val_acc: 0.5332\n",
      "Epoch 7/12\n",
      "25000/25000 [==============================] - 188s 8ms/step - loss: 0.6912 - acc: 0.5250 - val_loss: 0.6878 - val_acc: 0.5294\n",
      "Epoch 8/12\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6897 - acc: 0.5279 - val_loss: 0.6885 - val_acc: 0.5302\n",
      "Epoch 9/12\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6894 - acc: 0.5279 - val_loss: 0.6872 - val_acc: 0.5354\n",
      "Epoch 10/12\n",
      "25000/25000 [==============================] - 185s 7ms/step - loss: 0.6898 - acc: 0.5262 - val_loss: 0.6895 - val_acc: 0.5291\n",
      "Epoch 11/12\n",
      "25000/25000 [==============================] - 184s 7ms/step - loss: 0.6902 - acc: 0.5236 - val_loss: 0.6883 - val_acc: 0.5258\n",
      "Epoch 12/12\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6899 - acc: 0.5206 - val_loss: 0.6867 - val_acc: 0.5318\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Test score: 0.6866541689300537\n",
      "Test accuracy: 0.5318400000190735\n"
     ]
    }
   ],
   "source": [
    "#Modelling with RNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,batch_size=128,epochs=12,validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,batch_size=128)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.7209 - acc: 0.5047 - val_loss: 0.7111 - val_acc: 0.5068\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.7072 - acc: 0.4995 - val_loss: 0.6963 - val_acc: 0.5156\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6977 - acc: 0.5087 - val_loss: 0.6926 - val_acc: 0.5214\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6940 - acc: 0.5138 - val_loss: 0.6921 - val_acc: 0.5228\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6933 - acc: 0.5149 - val_loss: 0.6922 - val_acc: 0.5152\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6924 - acc: 0.5145 - val_loss: 0.6915 - val_acc: 0.5166\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6924 - acc: 0.5180 - val_loss: 0.6909 - val_acc: 0.5308\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6917 - acc: 0.5239 - val_loss: 0.6918 - val_acc: 0.5218\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.6924 - acc: 0.5170 - val_loss: 0.6912 - val_acc: 0.5214\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.6923 - acc: 0.5150 - val_loss: 0.6917 - val_acc: 0.5194\n",
      "25000/25000 [==============================] - 7s 269us/step\n",
      "0.514 5000 [411.70818584615404] 100 adam 0.51944\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.6671 - acc: 0.5913 - val_loss: 0.6661 - val_acc: 0.6185\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.6183 - acc: 0.6644 - val_loss: 0.6264 - val_acc: 0.6603\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.6023 - acc: 0.6778 - val_loss: 0.5981 - val_acc: 0.6774\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.5865 - acc: 0.6891 - val_loss: 0.5763 - val_acc: 0.6946\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.5787 - acc: 0.6935 - val_loss: 0.5753 - val_acc: 0.7014\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.5720 - acc: 0.7007 - val_loss: 0.5721 - val_acc: 0.7003\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.5709 - acc: 0.6994 - val_loss: 0.5771 - val_acc: 0.7021\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.5770 - acc: 0.6979 - val_loss: 0.5826 - val_acc: 0.6945\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.5711 - acc: 0.6994 - val_loss: 0.5736 - val_acc: 0.7013\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.5649 - acc: 0.7034 - val_loss: 0.5697 - val_acc: 0.7004\n",
      "25000/25000 [==============================] - 7s 269us/step\n",
      "0.514 5000 [411.70818584615404, 413.4159507692307] 100 rmsprop 0.7003999999809265\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 191s 8ms/step - loss: 0.7205 - acc: 0.5045 - val_loss: 0.7076 - val_acc: 0.5109\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.7041 - acc: 0.5082 - val_loss: 0.6965 - val_acc: 0.5179\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.6941 - acc: 0.5250 - val_loss: 0.6893 - val_acc: 0.5344\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.6917 - acc: 0.5222 - val_loss: 0.6877 - val_acc: 0.5384\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 189s 8ms/step - loss: 0.6896 - acc: 0.5252 - val_loss: 0.6869 - val_acc: 0.5348\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 190s 8ms/step - loss: 0.6877 - acc: 0.5317 - val_loss: 0.6870 - val_acc: 0.5348\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 197s 8ms/step - loss: 0.6878 - acc: 0.5315 - val_loss: 0.6856 - val_acc: 0.5377\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 201s 8ms/step - loss: 0.6863 - acc: 0.5394 - val_loss: 0.6851 - val_acc: 0.5378\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 208s 8ms/step - loss: 0.6875 - acc: 0.5327 - val_loss: 0.6852 - val_acc: 0.5439\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 213s 9ms/step - loss: 0.6868 - acc: 0.5402 - val_loss: 0.6841 - val_acc: 0.5475\n",
      "25000/25000 [==============================] - 39s 2ms/step\n",
      "0.50848 5000 [411.70818584615404, 413.4159507692307, 2013.4085456410257] 500 adam 0.5475199999809265\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 226s 9ms/step - loss: 0.5835 - acc: 0.6807 - val_loss: 0.4555 - val_acc: 0.7914\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 223s 9ms/step - loss: 0.4646 - acc: 0.7810 - val_loss: 0.4382 - val_acc: 0.8038\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 223s 9ms/step - loss: 0.4373 - acc: 0.7998 - val_loss: 0.4515 - val_acc: 0.7916\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 226s 9ms/step - loss: 0.4340 - acc: 0.8019 - val_loss: 0.4256 - val_acc: 0.8066\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 230s 9ms/step - loss: 0.4180 - acc: 0.8102 - val_loss: 0.4140 - val_acc: 0.8129\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 231s 9ms/step - loss: 0.3941 - acc: 0.8239 - val_loss: 0.3969 - val_acc: 0.8248\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 233s 9ms/step - loss: 0.3765 - acc: 0.8357 - val_loss: 0.3979 - val_acc: 0.8226\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 237s 9ms/step - loss: 0.3757 - acc: 0.8353 - val_loss: 0.4006 - val_acc: 0.8204\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 242s 10ms/step - loss: 0.3640 - acc: 0.8416 - val_loss: 0.3871 - val_acc: 0.8287\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 235s 9ms/step - loss: 0.3632 - acc: 0.8396 - val_loss: 0.3877 - val_acc: 0.8279\n",
      "25000/25000 [==============================] - 39s 2ms/step\n",
      "0.50848 5000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615] 500 rmsprop 0.8279200000381469\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 415s 17ms/step - loss: 0.7162 - acc: 0.5084 - val_loss: 0.7157 - val_acc: 0.5134\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 478s 19ms/step - loss: 0.7088 - acc: 0.4992 - val_loss: 0.7001 - val_acc: 0.5081\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 485s 19ms/step - loss: 0.6986 - acc: 0.5038 - val_loss: 0.6960 - val_acc: 0.5160\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 483s 19ms/step - loss: 0.6937 - acc: 0.5165 - val_loss: 0.6899 - val_acc: 0.5248\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 492s 20ms/step - loss: 0.6910 - acc: 0.5232 - val_loss: 0.6911 - val_acc: 0.5232\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 497s 20ms/step - loss: 0.6913 - acc: 0.5252 - val_loss: 0.6895 - val_acc: 0.5361\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 488s 20ms/step - loss: 0.6902 - acc: 0.5287 - val_loss: 0.6894 - val_acc: 0.5284\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 489s 20ms/step - loss: 0.6893 - acc: 0.5303 - val_loss: 0.6903 - val_acc: 0.5298\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 506s 20ms/step - loss: 0.6898 - acc: 0.5359 - val_loss: 0.6899 - val_acc: 0.5267\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 488s 20ms/step - loss: 0.6908 - acc: 0.5238 - val_loss: 0.6889 - val_acc: 0.5309\n",
      "25000/25000 [==============================] - 79s 3ms/step\n",
      "0.50696 5000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615, 5654.64530625641] 1000 adam 0.53092\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 439s 18ms/step - loss: 0.7110 - acc: 0.5227 - val_loss: 0.7067 - val_acc: 0.5192\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 471s 19ms/step - loss: 0.7103 - acc: 0.5082 - val_loss: 0.7112 - val_acc: 0.4958\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 481s 19ms/step - loss: 0.7015 - acc: 0.5126 - val_loss: 0.6962 - val_acc: 0.5166\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 480s 19ms/step - loss: 0.6949 - acc: 0.5138 - val_loss: 0.6941 - val_acc: 0.5152\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 497s 20ms/step - loss: 0.6949 - acc: 0.5148 - val_loss: 0.6917 - val_acc: 0.5218\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 479s 19ms/step - loss: 0.6909 - acc: 0.5272 - val_loss: 0.6884 - val_acc: 0.5345\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 477s 19ms/step - loss: 0.6880 - acc: 0.5374 - val_loss: 0.6867 - val_acc: 0.5443\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 481s 19ms/step - loss: 0.6860 - acc: 0.5428 - val_loss: 0.6834 - val_acc: 0.5546\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 497s 20ms/step - loss: 0.6856 - acc: 0.5514 - val_loss: 0.6818 - val_acc: 0.5609\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 479s 19ms/step - loss: 4.4132 - acc: 0.2684 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "25000/25000 [==============================] - 77s 3ms/step\n",
      "0.50696 5000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615, 5654.64530625641, 5614.690347897435] 1000 rmsprop 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 53s 2ms/step - loss: 6.7581 - acc: 0.0858 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 7.9712 - acc: 0.0000e+00 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "25000/25000 [==============================] - 9s 340us/step\n",
      "0.52088 10000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615, 5654.64530625641, 5614.690347897435, 529.1940139487197] 100 adam 0.0\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 54s 2ms/step - loss: 0.6766 - acc: 0.5716 - val_loss: 0.6544 - val_acc: 0.6167\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.6395 - acc: 0.6355 - val_loss: 0.6491 - val_acc: 0.6344\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 52s 2ms/step - loss: 0.6249 - acc: 0.6515 - val_loss: 0.6202 - val_acc: 0.6622\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 52s 2ms/step - loss: 0.6376 - acc: 0.6402 - val_loss: 0.6503 - val_acc: 0.6271\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 52s 2ms/step - loss: 0.6913 - acc: 0.5431 - val_loss: 0.7046 - val_acc: 0.5027\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 56s 2ms/step - loss: 0.7032 - acc: 0.5039 - val_loss: 0.7002 - val_acc: 0.5040\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 56s 2ms/step - loss: 0.6968 - acc: 0.5096 - val_loss: 0.6957 - val_acc: 0.5062\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 56s 2ms/step - loss: 0.6937 - acc: 0.5130 - val_loss: 0.6936 - val_acc: 0.5095\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 53s 2ms/step - loss: 0.6917 - acc: 0.5226 - val_loss: 0.6897 - val_acc: 0.5277\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 52s 2ms/step - loss: 0.6912 - acc: 0.5226 - val_loss: 0.6889 - val_acc: 0.5287\n",
      "25000/25000 [==============================] - 9s 345us/step\n",
      "0.52088 10000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615, 5654.64530625641, 5614.690347897435, 529.1940139487197, 550.4249505641055] 100 rmsprop 0.5287200000190735\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.7213 - acc: 0.5006 - val_loss: 0.7108 - val_acc: 0.5064\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.7046 - acc: 0.5075 - val_loss: 0.6955 - val_acc: 0.5200\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.6957 - acc: 0.5176 - val_loss: 0.6943 - val_acc: 0.5180\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.6930 - acc: 0.5223 - val_loss: 0.6907 - val_acc: 0.5216\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.6905 - acc: 0.5273 - val_loss: 0.6880 - val_acc: 0.5350\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.6900 - acc: 0.5325 - val_loss: 0.6882 - val_acc: 0.5337\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 262s 10ms/step - loss: 0.6898 - acc: 0.5312 - val_loss: 0.6867 - val_acc: 0.5425\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.6885 - acc: 0.5316 - val_loss: 0.6865 - val_acc: 0.5388\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.6880 - acc: 0.5330 - val_loss: 0.6863 - val_acc: 0.5365\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 247s 10ms/step - loss: 0.6896 - acc: 0.5293 - val_loss: 0.6863 - val_acc: 0.5394\n",
      "25000/25000 [==============================] - 39s 2ms/step\n",
      "0.51456 10000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615, 5654.64530625641, 5614.690347897435, 529.1940139487197, 550.4249505641055, 2538.4210391794877] 500 adam 0.5394400000190734\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.6404 - acc: 0.6218 - val_loss: 0.6484 - val_acc: 0.6155\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 245s 10ms/step - loss: 0.6645 - acc: 0.5914 - val_loss: 0.6852 - val_acc: 0.5525\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 242s 10ms/step - loss: 0.6968 - acc: 0.5316 - val_loss: 0.7028 - val_acc: 0.5171\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 257s 10ms/step - loss: 0.7014 - acc: 0.5083 - val_loss: 0.6971 - val_acc: 0.5203\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.6945 - acc: 0.5210 - val_loss: 0.6938 - val_acc: 0.5234\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 244s 10ms/step - loss: 0.6918 - acc: 0.5256 - val_loss: 0.6872 - val_acc: 0.5332\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 191s 8ms/step - loss: 0.6870 - acc: 0.5379 - val_loss: 0.6839 - val_acc: 0.5513\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 185s 7ms/step - loss: 0.6875 - acc: 0.5372 - val_loss: 0.6859 - val_acc: 0.5409\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6853 - acc: 0.5451 - val_loss: 0.6834 - val_acc: 0.5451\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 186s 7ms/step - loss: 0.6842 - acc: 0.5450 - val_loss: 0.6816 - val_acc: 0.5518\n",
      "25000/25000 [==============================] - 31s 1ms/step\n",
      "0.51456 10000 [411.70818584615404, 413.4159507692307, 2013.4085456410257, 2373.2523975384615, 5654.64530625641, 5614.690347897435, 529.1940139487197, 550.4249505641055, 2538.4210391794877, 2288.908318358972] 500 rmsprop 0.5517999999809265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 379s 15ms/step - loss: 0.7157 - acc: 0.5079 - val_loss: 0.7085 - val_acc: 0.5090\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 374s 15ms/step - loss: 0.7049 - acc: 0.5118 - val_loss: 0.7018 - val_acc: 0.5101\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 375s 15ms/step - loss: 0.6996 - acc: 0.5043 - val_loss: 0.6960 - val_acc: 0.5152\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 379s 15ms/step - loss: 0.6932 - acc: 0.5208 - val_loss: 0.6918 - val_acc: 0.5269\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 375s 15ms/step - loss: 0.6912 - acc: 0.5248 - val_loss: 0.6875 - val_acc: 0.5374\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 373s 15ms/step - loss: 0.6909 - acc: 0.5268 - val_loss: 0.6907 - val_acc: 0.5258\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 374s 15ms/step - loss: 0.6916 - acc: 0.5224 - val_loss: 0.6889 - val_acc: 0.5304\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 379s 15ms/step - loss: 0.6905 - acc: 0.5294 - val_loss: 0.6868 - val_acc: 0.5366\n",
      "Epoch 9/10\n",
      " 5120/25000 [=====>........................] - ETA: 4:12 - loss: 0.6927 - acc: 0.5127"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-10b5e10face6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 908\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    909\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1143\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1144\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1324\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1325\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1328\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1313\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1315\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1421\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1422\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1423\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Experimenting with different number of featurs, words in each review and optimizer. \n",
    "#Calculating the logit accuracy and rnn accuracy for each combination\n",
    "\n",
    "logit_score=[]\n",
    "features=[]\n",
    "opti=[]\n",
    "words=[]\n",
    "time_taken=[]\n",
    "rnn_score=[]\n",
    "rnn_acc=[]\n",
    "\n",
    "top_words=[5000,10000,20000]\n",
    "max_words = [100,500,1000]\n",
    "optimizers=['adam','rmsprop']\n",
    "\n",
    "for top_word in top_words:\n",
    "    for max_word in max_words:\n",
    "        for optimizer in optimizers:\n",
    "            import time\n",
    "            time_start=time.clock()\n",
    "            #Loading the required data\n",
    "            (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_word)\n",
    "            X_train = sequence.pad_sequences(X_train, maxlen=max_word)\n",
    "            X_test = sequence.pad_sequences(X_test, maxlen=max_word)\n",
    "            \n",
    "            #Modelling with logistic regression\n",
    "            model1 = LogisticRegression()\n",
    "            model1.fit(X_train,y_train)\n",
    "            y_predict=model1.predict(X_test)\n",
    "            accuracy_test=accuracy_score(y_test,y_predict)\n",
    "            \n",
    "            \n",
    "            #Modelling with RNN\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(max_word, 128))\n",
    "            model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "            model.fit(X_train, y_train,batch_size=128,epochs=10,validation_data=(X_test, y_test))\n",
    "            score, acc = model.evaluate(X_test, y_test,batch_size=128)\n",
    "            \n",
    "            time_elapsed = (time.clock() - time_start)\n",
    "            \n",
    "            time_taken.append(time_elapsed)\n",
    "            logit_score.append(accuracy_test)\n",
    "            features.append(top_word)\n",
    "            words.append(max_word)\n",
    "            opti.append(optimizer)\n",
    "            rnn_acc.append(acc)\n",
    "            rnn_score.append(score)\n",
    "            \n",
    "            print(accuracy_test,top_word,time_taken,max_word,optimizer,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['features'] = features\n",
    "results['no_words'] = words\n",
    "results['optimizer'] = opti\n",
    "results['logitstic_reg_acc'] = logit_score\n",
    "results['rnn_acc']=rnn_acc\n",
    "results['rnn_score']=rnn_score\n",
    "results['time_taken']=time_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>no_words</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>logitstic_reg_acc</th>\n",
       "      <th>rnn_acc</th>\n",
       "      <th>rnn_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000</td>\n",
       "      <td>500</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.50848</td>\n",
       "      <td>0.82792</td>\n",
       "      <td>0.387746</td>\n",
       "      <td>2373.252398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "      <td>100</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.51400</td>\n",
       "      <td>0.70040</td>\n",
       "      <td>0.569705</td>\n",
       "      <td>413.415951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>500</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.51456</td>\n",
       "      <td>0.55180</td>\n",
       "      <td>0.681592</td>\n",
       "      <td>2288.908318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5000</td>\n",
       "      <td>500</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.50848</td>\n",
       "      <td>0.54752</td>\n",
       "      <td>0.684054</td>\n",
       "      <td>2013.408546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000</td>\n",
       "      <td>500</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.51456</td>\n",
       "      <td>0.53944</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>2538.421039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.50696</td>\n",
       "      <td>0.53092</td>\n",
       "      <td>0.688904</td>\n",
       "      <td>5654.645306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.52088</td>\n",
       "      <td>0.52872</td>\n",
       "      <td>0.688931</td>\n",
       "      <td>550.424951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.51400</td>\n",
       "      <td>0.51944</td>\n",
       "      <td>0.691671</td>\n",
       "      <td>411.708186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.50696</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.971193</td>\n",
       "      <td>5614.690348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.52088</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.971193</td>\n",
       "      <td>529.194014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features  no_words optimizer  logitstic_reg_acc  rnn_acc  rnn_score  \\\n",
       "3      5000       500   rmsprop            0.50848  0.82792   0.387746   \n",
       "1      5000       100   rmsprop            0.51400  0.70040   0.569705   \n",
       "9     10000       500   rmsprop            0.51456  0.55180   0.681592   \n",
       "2      5000       500      adam            0.50848  0.54752   0.684054   \n",
       "8     10000       500      adam            0.51456  0.53944   0.686275   \n",
       "4      5000      1000      adam            0.50696  0.53092   0.688904   \n",
       "7     10000       100   rmsprop            0.52088  0.52872   0.688931   \n",
       "0      5000       100      adam            0.51400  0.51944   0.691671   \n",
       "5      5000      1000   rmsprop            0.50696  0.00000   7.971193   \n",
       "6     10000       100      adam            0.52088  0.00000   7.971193   \n",
       "\n",
       "    time_taken  \n",
       "3  2373.252398  \n",
       "1   413.415951  \n",
       "9  2288.908318  \n",
       "2  2013.408546  \n",
       "8  2538.421039  \n",
       "4  5654.645306  \n",
       "7   550.424951  \n",
       "0   411.708186  \n",
       "5  5614.690348  \n",
       "6   529.194014  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe showing the top rnn accuracy scores\n",
    "(results.sort_values(by='rnn_acc',ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "1. Logistic model gave an accuracy of around 50% for all models. NaiaveBayes also has the same accuracy.\n",
    "2. The best accuracy score of 82.27% is achieved by considering the top 5000 words with first 500 words in each review.\n",
    "Refrences:\n",
    "1. https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n",
    "2. http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
